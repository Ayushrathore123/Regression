{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d43833",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158878b4",
   "metadata": {},
   "source": [
    "R-squared (coefficient of determination) is a statistical measure that represents the proportion of the variance in the dependent variable that can be explained by the independent variables in a linear regression model. It quantifies the goodness of fit of the model. R-squared values range from 0 to 1, where 1 indicates a perfect fit. It is calculated as the ratio of the explained sum of squares (ESS) to the total sum of squares (TSS). A higher R-squared indicates that the model explains a larger portion of the variability in the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3feacb",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc12c6",
   "metadata": {},
   "source": [
    "It measures the proportion of variation explained by only those independent variables that really help in explaining the dependent variable. It penalizes you for adding independent variables that do not help in predicting the dependent variable in regression analysis.Adjusted R-squared differs from the regular R-squared by considering the number of predictors in a linear regression model. While regular R-squared can increase even with the addition of irrelevant predictors, adjusted R-squared penalizes the inclusion of unnecessary predictors. It provides a more reliable measure of the model's fit and helps prevent overfitting by accounting for the model's complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e93fb2",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c438f",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when comparing models with different numbers of predictors. It helps to assess the model's goodness of fit while considering the complexity introduced by the number of predictors, providing a fairer comparison and mitigating the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ddee9",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286309fd",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics in regression analysis to evaluate the performance of predictive models.\n",
    "\n",
    "RMSE: It is the square root of the average of squared differences between predicted and actual values. It represents the standard deviation of the residuals and provides a measure of how close the predicted values are to the actual values.\n",
    "\n",
    "MSE: It is the average of squared differences between predicted and actual values. It quantifies the average squared difference between predicted and actual values, emphasizing larger errors more than MAE.\n",
    "\n",
    "MAE: It is the average of absolute differences between predicted and actual values. It provides the average magnitude of errors without considering their direction, making it less sensitive to outliers compared to RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8973dc4",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7969a8",
   "metadata": {},
   "source": [
    "## MSE:\n",
    "Advantage:\n",
    "- 1 it is differentiable\n",
    "- 2 it has one local and global minima\n",
    "\n",
    "Disadvantage\n",
    "- 1. not robust to outliers\n",
    "- 2. it is not in same unit\n",
    "## MAE:\n",
    "Advantage:\n",
    "- 1.Robust to outlier\n",
    "- 2. it will be in same unit\n",
    "\n",
    "Disadvantage:\n",
    "- 1.convergence usually takes time ,optimation is complex\n",
    "- 2.Time consuming\n",
    "## RMSE:\n",
    "Advantage:\n",
    "- 1.it has same unit\n",
    "- 2. it is differential\n",
    "\n",
    "Disadvantage:\n",
    "- 1 not robust to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766a238",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb8b8c",
   "metadata": {},
   "source": [
    "Lasso regularization is a method used in linear regression to add a penalty term to the loss function, encouraging sparsity by shrinking less important coefficients to zero. It differs from Ridge regularization by performing both feature selection and parameter shrinkage, making it more appropriate when dealing with high-dimensional datasets and when there is a belief that only a subset of features is truly relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdb704",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec02251",
   "metadata": {},
   "source": [
    "Regularized linear models help prevent overfitting by adding a penalty term to the loss function, discouraging complex models with high coefficients. For example, in Ridge regression, the penalty term limits the magnitude of the coefficients, reducing their impact on the model's predictions and promoting simpler, more generalized models that are less prone to overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745f7bc",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c48072",
   "metadata": {},
   "source": [
    "They assume a linear relationship between predictors and the target variable, which may not hold in all cases.\n",
    "\n",
    "They rely on the selection of an appropriate regularization parameter, which can be challenging and may impact model performance.\n",
    "\n",
    "Regularized models may struggle with highly correlated predictors or when dealing with large datasets due to increased computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901460b",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0f8b6",
   "metadata": {},
   "source": [
    "we would choose Model B as the better performer because it has a lower MAE of 8 compared to Model A's RMSE of 10. MAE represents the average magnitude of errors and is less sensitive to outliers, making it a good choice for comparing the overall accuracy of the models.\n",
    "\n",
    "However, it's important to note that the choice of metric depends on the specific requirements of the problem. RMSE gives higher weight to larger errors, which can be useful when large errors are of greater concern. Additionally, the choice of metric does not consider other aspects of the models' performance, such as bias or distributional properties, so it's important to consider the limitations and assumptions of the metric being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ece1c",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6c71f",
   "metadata": {},
   "source": [
    "Model A (Ridge regularization) with a regularization parameter of 0.1 may be preferable if we want to shrink the coefficients while maintaining all features' importance.\n",
    "\n",
    "Model B (Lasso regularization) with a regularization parameter of 0.5 may be more appropriate if feature selection is desired, as it encourages sparsity and may result in some coefficients being reduced to zero.\n",
    "Trade-offs and limitations include the need to carefully select the regularization parameter and the potential loss of interpretability when using Lasso regularization due to feature selection. Additionally, regularized linear models assume a linear relationship, which may not hold in all cases. It is essential to consider these factors when choosing the regularization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320cc5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
