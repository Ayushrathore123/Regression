{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6047b01b",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc4239",
   "metadata": {},
   "source": [
    "Linear regression is a simple statistical  method that is uesd to establish a relationship between dependent variable and one or more independent variable while multiple regression is used for two or more multiple regression\n",
    "\n",
    "In simple linear regression, a straight line is fitted to a set of data points to find the best-fit line that describes the\n",
    "relationship between the dependent variable and the independent variable. For example, if we want to predict the sales of a \n",
    "product based on its price, we can use simple linear regression to establish the relationship between the price and the sales\n",
    "\n",
    "On the other hand, multiple linear regression involves more than one independent variable. For example, if we want to predict\n",
    "the price of a house based on its size, location, and number of bedrooms, we can use multiple linear regression to establish \n",
    "the relationship between these variables and the price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737509d",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4627db",
   "metadata": {},
   "source": [
    "Linearity: The relationship between the dependent variable and the independent variables is linear.\n",
    "\n",
    "Independence: The observations are independent of each other.\n",
    "\n",
    "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable.\n",
    "\n",
    "Normality: The residuals are normally distributed.\n",
    "\n",
    "No multicollinearity: The independent variables are not highly correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4609ba",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe6f1a",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept are used to describe the relationship between the dependent \n",
    "variable and the independent variable(s). The intercept is the value of the dependent variable when all the independent \n",
    "variables are equal to zero. The slope is the change in the dependent variable for every one-unit change in the independent \n",
    "variable.\n",
    "\n",
    "To illustrate this concept, let's consider a real-world scenario where we want to predict the price of a used car based on its\n",
    "mileage. We can use a linear regression model to establish the relationship between the price and mileage of the car.\n",
    "\n",
    "The model can be expressed as:\n",
    "\n",
    "Price = intercept + slope x Mileage + error\n",
    "\n",
    "The intercept represents the base price of the car, and the slope represents the change in the price of the car for every \n",
    "one-unit increase in mileage.\n",
    "\n",
    "For example, let's say the intercept is $5,000, and the slope is -0.05. This means that the base price of the car is $5,000, \n",
    "and the price decreases by $0.05 for every one-mile increase in mileage. So, if a car has 100,000 miles on it, we can estimate its price as:\n",
    "\n",
    "Price = 5,000 - 0.05 x 100,000 = $0\n",
    "\n",
    "This estimate of $0 indicates that the car has zero value, which is not realistic. In this case, we may need to examine the\n",
    "model and data more closely to see if there are any issues with the assumptions or outliers that may be affecting the results.\n",
    "\n",
    "In summary, the intercept and slope in a linear regression model provide important information about the relationship between \n",
    "the dependent variable and independent variable(s), and can be used to make predictions and interpret the results of the\n",
    "analysis.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e50ea2",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7cd37",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative optimization algorithm used to minimize the cost function of a model. It is\n",
    "commonly used in machine learning to update the model parameters in order to improve its performance.\n",
    "\n",
    "The basic idea behind gradient descent is to compute the gradient of the cost function with respect to the model parameters, \n",
    "and then update the parameters in the opposite direction of the gradient, with the aim of finding the minimum of the cost \n",
    "function.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "Initialize the model parameters with some arbitrary values.\n",
    "\n",
    "Calculate the cost function using the current parameter values.\n",
    "\n",
    "Calculate the gradient of the cost function with respect to each parameter.\n",
    "\n",
    "Update each parameter by subtracting the gradient multiplied by a learning rate, which is a hyperparameter that determines \n",
    "the size of the update. The learning rate is typically set to a small value to avoid overshooting the minimum of the cost\n",
    "function.\n",
    "\n",
    "Repeat steps 2-4 until the cost function reaches a minimum or a stopping criterion is met.\n",
    "\n",
    "Gradient descent can be applied to different types of machine learning models, such as linear regression, logistic regression, and neural networks. It is especially useful in large-scale machine learning problems where the cost function is high-dimensional and the computational cost of computing the gradient is prohibitive.\n",
    "\n",
    "In summary, gradient descent is an optimization algorithm used to update the model parameters of a machine learning model \n",
    "in order to minimize the cost function. By iteratively updating the parameters in the direction of the negative gradient, \n",
    "the algorithm can find the minimum of the cost function and improve the performance of the model.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e2921",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca4e2f",
   "metadata": {},
   "source": [
    "Multiple linear regression is a statistical technique used to model the relationship between a dependent variable and \n",
    "two or more independent variables. In this model, the dependent variable is predicted based on the values of several \n",
    "independent variables, rather than just one as in simple linear regression.\n",
    "\n",
    "The multiple linear regression model can be expressed as:\n",
    "\n",
    "y = β0 + β1x1 + β2x2 + ... + βnxn + ε\n",
    "\n",
    "where:\n",
    "\n",
    "y is the dependent variable\n",
    "x1, x2, ..., xn are the independent variables\n",
    "β0 is the intercept\n",
    "β1, β2, ..., βn are the coefficients for each independent variable\n",
    "ε is the error term, which represents the random variation in the dependent variable that is not explained by the independent\n",
    "variables.\n",
    "The coefficients β1, β2, ..., βn represent the change in the dependent variable for every one-unit change in the corresponding independent variable, while holding all other independent variables constant.\n",
    "\n",
    "Compared to simple linear regression, multiple linear regression allows us to model more complex relationships between the\n",
    "dependent variable and independent variables. For example, we can use multiple linear regression to model the relationship \n",
    "between a person's height, weight, and age, and their risk of developing a certain disease.\n",
    "\n",
    "However, multiple linear regression also requires more assumptions and more data than simple linear regression, as there \n",
    "are more variables and interactions to consider. It is also more susceptible to overfitting, which occurs when the model \n",
    "fits the training data too closely and does not generalize well to new data.\n",
    "\n",
    "In summary, multiple linear regression is a more complex version of simple linear regression that allows us to model the \n",
    "relationship between a dependent variable and multiple independent variables. It requires more assumptions and data, but \n",
    "can capture more complex relationships between variables.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45447c5",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42fd83",
   "metadata": {},
   "source": [
    "Multicollinearity refers to the presence of high correlation among independent variables in a multiple linear regression model. It can cause issues in the model such as unstable coefficients and unreliable statistical inferences.\n",
    "\n",
    "To detect multicollinearity, you can calculate the correlation matrix of the independent variables and look for high correlation coefficients (close to +1 or -1). Additionally, you can calculate the variance inflation factor (VIF) for each independent variable, where values above 5 or 10 indicate potential multicollinearity.\n",
    "\n",
    "To address multicollinearity, you have a few options:\n",
    "\n",
    "- Remove one or more correlated independent variables from the model if they are not essential.\n",
    "- Combine correlated variables into a single variable through dimensionality reduction techniques like principal component analysis (PCA).\n",
    "- Collect more data to reduce the impact of multicollinearity.\n",
    "- Use regularization techniques like ridge regression or Lasso regression, which can help mitigate the effects of multicollinearity by adding a penalty term to the regression equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea85cba4",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52ce29",
   "metadata": {},
   "source": [
    "Polynomial regression is an extension of linear regression that allows for modeling nonlinear relationships between the independent and dependent variables. Instead of fitting a straight line, polynomial regression fits a curve by including polynomial terms (e.g., squared or cubed terms) of the independent variable in the model equation. This enables capturing more complex patterns in the data. The key difference from linear regression is that it introduces higher-degree polynomial terms, allowing for a flexible and curved relationship between the variables, whereas linear regression assumes a linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee98d9",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bff5e",
   "metadata": {},
   "source": [
    "Advantages of polynomial regression:\n",
    "\n",
    "- Flexibility: Polynomial regression can capture nonlinear relationships between variables, allowing for more complex and curved patterns in the data.\n",
    "- Improved fit: Polynomial regression can provide a better fit to the data compared to linear regression when the relationship is not strictly linear.\n",
    "- Variable interaction: Polynomial regression can account for interactions between variables by including interaction terms.\n",
    " \n",
    "Disadvantages of polynomial regression:\n",
    "\n",
    " - Overfitting: Higher-degree polynomial terms can lead to overfitting if the model captures noise or irrelevant patterns in the data.\n",
    " - Complexity: Polynomial regression introduces additional complexity to the model, making it more challenging to interpret and understand.\n",
    "- Extrapolation: Extrapolating beyond the range of the observed data in polynomial regression can be unreliable and lead to unreliable predictions.\n",
    "\n",
    "Polynomial regression is preferred when:\n",
    "\n",
    "- There is evidence or prior knowledge suggesting a nonlinear relationship between the variables.\n",
    "- The relationship between the variables cannot be adequately captured by a linear model.\n",
    "- There is a need for improved model fit or capturing complex patterns in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ccb87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
